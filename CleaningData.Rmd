---
title: "Loading and cleaning data"
author: "Russell Dinnage and Anna Simonsen"
output:
  html_document:
    highlight: pygments
  rmdformats::html_docco:
    fig_width: 6
    fig_height: 6
    highlight: pygments
bibliography: "E:/Projects/GlobalNfix/refs/GNFD.bib"
---

```{r knitr_init, echo=FALSE, results="asis", cache=FALSE}
library(knitr)

## Global options
options(max.print="75")
opts_chunk$set(fig.path="out/",
               echo=TRUE,
	       cache=TRUE,
               cache.path="cache/",
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

First, we need to download the @Werner2014 data set from Dryad if it is not already dowloaded, and then load it into R. We will also save a list of the species names from this dataset into the `data` directory of this package, so we can easily access it in other analyses.

```{r}
if (!file.exists("inst/extdata/Werner_NFix.csv")){
  download.file("https://datadryad.org/bitstream/handle/10255/dryad.64120/Werner_NFix.csv",
                "inst/extdata/Werner_NFix.csv", "curl")
}
N_fix_data <- read.csv("inst/extdata/Werner_NFix.csv", stringsAsFactors = FALSE)
species <- N_fix_data$Species
legume_species <- N_fix_data$Species[N_fix_data$Family == "Fabaceae"]
other_species <- N_fix_data$Species[N_fix_data$Family != "Fabaceae"]
save(species, legume_species, other_species, file = "data/species.rda")
write.table(N_fix_data, file = "data/N_fix_data.csv", row.names = FALSE, sep = ";")
```

It is possible to use R packages to download GBIF data from GBIF directly, but for large queries it is often more efficient to download a large dataset from the GBIF web portal, and analyze that in R. We have already downloaded all of the georeferenced records in the family Fabaceae. The file has more than 5 million records, and is about 3.8 GB in size. To load it into R, we will need the [`LaF`](http://cran.r-project.org/web/packages/LaF/index.html) package, which can access an ASCII file in R without loading the entire file into physical memory. 

In order to use `LaF`, the datafile must be formatted correctly. Unforunately, the raw GBIF data files usually contain quotes with fields, which is not compatible with `LaF`. I have not figured out a cross-platform solution to this problem yet, but using linux we can use basic commands to remove all single and double quotes from the data file as follows (this may take a few minutes):

```
if [ ! -f ./data/gbif_occ.txt ]; then sed 's/\"//g' data/occurrence.txt > data/gbif_occ.txt; fi
```

First we will read in the first line and figure out what our columns are.

```{r}
library("LaF")
frow <- as.vector(read.table("bigdata/gbif_occ.txt", sep = "\t", nrows = 1,
                             stringsAsFactors = FALSE)[1, ])
paste(frow, collapse = ", ")
```

That is a lot of columns! `LaF` requires that you specify the type of all the columns, but we only care about a few of them. We will make sure these columns have the correct type. for the rest we will just import as 'strings'. We are interested in the Species names, and the Latitudes, and the Longitudes. We will also use the GBIF ID column, in case we need to come back later an get additional data.

```{r}
col_type <- vector(mode= "character", length = length(frow))
names(col_type) <- as.character(frow)
col_need <- c("gbifID", "decimalLatitude", "decimalLongitude", "genus", "scientificName", "species", "family", "order", "speciesKey")
col_type[col_need] <- c("integer", "double", "double", "string", "string", "string", "string", "string", "integer")
col_type[col_type == ""] <- "string"
gbif_dat <- laf_open_csv("bigdata/gbif_occ.txt", column_types = col_type, column_names = as.character(frow), sep = "\t", skip = 1)
```

Now that the data is loaded using `LaF`, we can extract just the columns we want, which should be able to fit into memory (Only one way to find out for sure!).

```{r}
gbif_dat_reduced <- gbif_dat[ , col_need]
## How big is it?
print(object.size(gbif_dat_reduced), units = "Mb")
```

As a basic sanity check, let's load up some `dplyr` and `ggplot2` and see whether the data looks like it is actually data on a world map. But first we should reduce it so that we are only plotting the species which actually occur in the Werner et al. database of nitrogen fixation.

```{r}
library(dplyr)
library(ggplot2)
## convert to a dplyr table
gbif_dat_reduced <- gbif_dat_reduced %>% tbl_df
red <- gbif_dat_reduced %>%
  filter(gbif_dat_reduced$species %in% N_fix_data$Species)
red
```

Let's plot it and see if it looks vaguely world shaped! (this might take awhile; we are plotting 11 million points!)

```{r}
p <- ggplot(red, aes(decimalLongitude, decimalLatitude)) + 
  geom_point(alpha = 0.1)
p
```

Let's see how many of the @Werner2014 species have corresponding GBIF records.

```{r}
legumes_in_GBIF <- N_fix_data$Species[N_fix_data$Species %in% gbif_dat_reduced$species]
length(legumes_in_GBIF)
```

So there are `r length(legumes_in_GBIF)` legumes that are in both the @Werner2014 nitrogen fixation dataset and the GBIF spatial dataset. That is `r (length(legumes_in_GBIF)/nrow(N_fix_data))*100`%. How many of those were analysed in @Werner2014 (which means they have a phylogeny and have been error checked)?

```{r}
fulldat_legumes <- sum(N_fix_data$Analysed_Werner_etal.[N_fix_data$Species %in% gbif_dat_reduced$species] == "Analysed")
fulldat_legumes
```

So that is only `r (fulldat_legumes/nrow(N_fix_data))*100`%. This is still a good number. But it suggests we may want to use the whole dataset, but conduct sensitivity analyses to see how are results are changed by simulating error in different ways. This might be a good place to use a "virtual ecologist" [@Zurell2010] - style analysis, actually!

Let's map out all the occurrences of species that were analysed in @Werner2014.

```{r mappy}
library(maps)
reduced_data <- gbif_dat_reduced %>% filter(species %in% N_fix_data$Species[N_fix_data$Analysed_Werner_etal. == "Analysed"])
all_states <- map_data("world")
#plot all states with ggplot
p <- ggplot()
p <- p + geom_polygon(data=all_states, aes(x=long, y=lat, group = group),colour="white", fill="grey10") + geom_point(data = reduced_data, aes(decimalLongitude, decimalLatitude), alpha = 0.1, colour = "red", size = 0.5)
p
```

It turns out we still have about 10 million occurrence records for the remaining species and good latitudinal coverage. It would appear that those legumes that occur commonly in genbank records (and so appear in the phylogeny used by @Werner2014) also have good GBIF records. I suppose this makes sense.

The next step will be to look for and clean-up any obvious errors in the GBIF data. For example, there are a number of records at lat-long of exactly 0. This usually mean the data is missing and the recorders put zeroes instead of missing data for the coordinates. We will get rid of these. Also any records which are not on any land-masses (we are not interested in those pesky marine legumes in this study). We should also double-check that there are not any errors in the species names (there shouldn't be).

For now, we will just remove the coordinates with exactly zero in the Latitude or Longitude, and remove the oceanic occurrences after we aggregate the data (because most of the points in the ocean will be just off-shore, suggesting they are just due to small errors. Then we will merge the GBIF data with the nitrogen fixation data and save it to our data directory for later use.

```{r rm_zeroes}
library(GlobalNfix)
data(N_fix_data) ## load data which is now part of GlobalNfix package
gbif_legumes <- gbif_dat_reduced %>%
  filter(gbif_dat_reduced$species %in% N_fix_data$Species) %>%
  filter(decimalLatitude != 0 & decimalLongitude != 0) %>%
  select(gbifID, decimalLatitude, decimalLongitude, genus, scientificName, 
         Species = species, family, order, speciesKey)
full_legume_data <- left_join(gbif_legumes, N_fix_data)
full_legume_data %>% select(decimalLatitude, decimalLongitude, Species, Data_fixing)
save(full_legume_data, file = "data/full_legume_data.rda")
```

Map of full data (including legume species not analysed in @Werner2014):

```{r map_full_data}
p <- ggplot()
p <- p + geom_polygon(data=all_states, aes(x=long, y=lat, group = group),colour="white", fill="grey10") + geom_point(data = full_legume_data, aes(decimalLongitude, decimalLatitude), alpha = 0.1, colour = "red", size = 0.5)
p
```

#References
