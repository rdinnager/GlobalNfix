---
title: "ILDIS Webscraper"
author: "Russell Dinnage and Anna Simonsen"
date: "30/07/2014"
output: 
  html_document:
    highlight: pygments
bibliography: "/home/din02g/Google Drive/References/GNFD.bib"
---

```{r knitr_init, echo=FALSE, results="asis", cache=FALSE}
library(knitr)

## Global options
options(max.print="75")
opts_chunk$set(fig.path="out/",
               echo=TRUE,
         cache=TRUE,
               cache.path="cache/",
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

Here we are going to try and build a simple webscraper to grab geographic data about legumes from the International Legume Database and Information Service ([ILDIS](http://www.ildis.org)). First let's see if we can access an html file using the species name.

```{r build_url}
species <- c("Medicago", "lupulina")
spec_url <- sprintf("http://www.ildis.org/LegumeWeb?version~10.01&LegumeWeb&genus~%s&species~%s",
                    species[1], species[2])
html_file <- paste0("/home/din02g/ILDIS_html/", paste(species, collapse = "_"), ".html")
download.file(spec_url, html_file, method = "curl")
spec_html <- readLines(html_file)
```

Next we need to get the data we need. Luckily, it is contained on a single line of the `.html`, after the line with "Geographical records". Once we get this line, we just need to strip out all html tags, and the footnote numbers, before we can process the geographical area names.

```{r clean_html}
geo_recs <- grep("Geographical records", spec_html, fixed = TRUE) + 1
clean_recs <- gsub("<sup>.*?</sup>", "", spec_html[geo_recs]) ## remove footnotes
clean_recs <- gsub("<p>", "\n", clean_recs, fixed = TRUE) ## remove paragraph breaks
clean_recs <- gsub("*", "", clean_recs, fixed = TRUE) ## remove asterisks
clean_recs <- gsub("<.*?>", "", clean_recs) ## remove html tags
clean_recs <- gsub("&.*?;", "", clean_recs) ## remove special html characters
clean_recs <- gsub("(N = native, I = introduced, U = uncertain status)\n", "", clean_recs, fixed= TRUE)
clean_recs
```

Let's just check if that works on another legume, chosen haphazardly.

```{r try_again}
species2 <- c("Acacia", "acuminata")
spec_url <- sprintf("http://www.ildis.org/LegumeWeb?version~10.01&LegumeWeb&genus~%s&species~%s",
                    species2[1], species2[2])
html_file <- paste0("/home/din02g/ILDIS_html/", paste(species2, collapse = "_"), ".html")
download.file(spec_url, html_file, method = "curl")
spec_html <- readLines(html_file)
geo_recs <- grep("Geographical records", spec_html, fixed = TRUE) + 1
clean_recs <- gsub("<sup>.*?</sup>", "", spec_html[geo_recs])
clean_recs <- gsub("<p>", "\n", clean_recs, fixed = TRUE)
clean_recs <- gsub("*", "", clean_recs, fixed = TRUE)
clean_recs <- gsub("<.*?>", "", clean_recs) ## remove html tags
clean_recs <- gsub("&.*?;", "", clean_recs) ## remove special html characters
clean_recs <- gsub("(N = native, I = introduced, U = uncertain status)\n", "", clean_recs, fixed= TRUE)
clean_recs
```

Looks good. Next we will roll this up into a function and start pinging ILDIS for all of the species in the @Werner2014 nitrogen fixation dataset. The only issue might be if a name ever comes up with multiple entries in ILDIS, in which case this script will not work properly. The following is an example where we get multiple hits, and a solution to the problem.

```{r mult_hits}
species <- c("Trifolium", "alexandrinum")
spec_url <- sprintf("http://www.ildis.org/LegumeWeb?version~10.01&LegumeWeb&genus~%s&species~%s",
                    species[1], species[2])
html_file <- paste0("/home/din02g/ILDIS_html/temp/", paste(species, collapse = "_"), ".html")
download.file(spec_url, html_file, method = "curl")
search_html <- readLines(html_file)
accept_name_line <- search_html[grep("accepted</td>", spec_html)]
accept_name_url <- regmatches(accept_name_line, regexpr("<a href=.*?>", accept_name_line))
accept_name_url <- sub("<a href=\"", "", accept_name_url)
accept_name_url <- sub("\">", "", accept_name_url)
accept_name_url
html_file <- paste0("/home/din02g/ILDIS_html/", paste(species, collapse = "_"), ".html")
download.file(accept_name_url, html_file, method = "curl")
```

Then we need to put these region names into a data structure and link them up to the geographic regional borders in the Taxonomic Database Working Group scheme ([TDWG](http://www.kew.org/science-research-data/kew-in-depth/gis/resources-and-publications/data/tdwg/index.htm)). Will work on that once we have all of the data scraped.

Okay, we have made some functions to scrape from ILDIS. To access them we need to load the project package. Then we will go through the @Werner2014 species and download html files for each.

```{r ILDIS_html_down, eval=FALSE}
library(GlobalNfix)
data(legume_species)
for (i in legume_species) {
  download_ILDIS(species = i, hdir = "/home/din02g/ILDIS_html/")
  Sys.sleep(1)
}
```

Okay, now that that is done, we can go through the `.html` files one by one and extract their distribution data.

```{r extract_dist_data}
html_path <- "/home/annarussell/Data/GlobalNfix/ILDIS_html"
html_files <- list.files(html_path, full.names = TRUE)
extract_regions <- function(html_file) {
  spec_html <- readLines(html_file)
  geo_recs <- grep("Geographical records", spec_html, fixed = TRUE) + 1
  clean_recs <- gsub("<sup>.*?</sup>", "", spec_html[geo_recs])
  clean_recs <- gsub("<p>", "\n", clean_recs, fixed = TRUE)
  clean_recs <- gsub("*", "", clean_recs, fixed = TRUE)
  clean_recs <- gsub("<.*?>", "", clean_recs) ## remove html tags
  clean_recs <- gsub("&.*?;", "", clean_recs) ## remove special html characters
  clean_recs <- gsub("(N = native, I = introduced, U = uncertain status)\n", "", clean_recs, fixed= TRUE)
  clean_recs  
}
regions <- sapply(html_files, extract_regions)
regions[1:10]
```

Now we just have to reformat these strings into some usable data. First we will split each string by newline characters, then separate the large regions (ending with a colon) from the sub-regions. Then we can extract the native vs. introduced designation. we will use _Abarema barbouriana_ as a test case.

```{r extract_data}
test_case <- gsub(", +", ",", regions[[2]])
test_case <- gsub(": +", ":", test_case)
test_case
test_reg <- strsplit(test_case, "\n", fixed = TRUE)
test_reg
test_reg <- strsplit(test_reg[[1]], ":", fixed = TRUE)
test_reg
test_reg <- lapply(test_reg, function(x) cbind(x[1], strsplit(x[2], ",", fixed = TRUE)[[1]]))
test_reg
test_reg <- do.call(rbind, test_reg)
colnames(test_reg) <- c("Region", "Subregion")
test_reg <- data.frame(test_reg, stringsAsFactors = FALSE)
test_reg$Subregion[is.na(test_reg$Subregion)] <- test_reg$Region[is.na(test_reg$Subregion)] 
find <- regexpr("\\((.+)\\)", test_reg$Subregion)
test_reg$Status <- NA
test_reg$Status[find != -1] <- gsub("\\(|\\)", "", regmatches(test_reg$Subregion, find))
test_reg$Subregion <- gsub("\\((.+)\\)", "", test_reg$Subregion)
test_reg
```

Let's put that into a function and get it done for all of the legume species.

```{r reg_func}
grab_dist_data <- function(region){
  test_case <- gsub(", +", ",", region)
  test_case <- gsub(": +", ":", test_case)
  test_reg <- strsplit(test_case, "\n", fixed = TRUE)
  test_reg <- strsplit(test_reg[[1]], ":", fixed = TRUE)
  test_reg <- lapply(test_reg, function(x) cbind(x[1], strsplit(x[2], ",", fixed = TRUE)[[1]]))
  test_reg <- do.call(rbind, test_reg)
  colnames(test_reg) <- c("Region", "Subregion")
  test_reg <- data.frame(test_reg, stringsAsFactors = FALSE)
  test_reg$Subregion[is.na(test_reg$Subregion)] <- test_reg$Region[is.na(test_reg$Subregion)] 
  find <- regexpr("\\((.+)\\)", test_reg$Subregion)
  test_reg$Status <- NA
  test_reg$Status[find != -1] <- gsub("\\(|\\)", "", regmatches(test_reg$Subregion, find))
  test_reg$Subregion <- gsub("\\((.+)\\)", "", test_reg$Subregion)
  test_reg
}
total_regions <- list()
spec_name <- gsub(".html", "", regmatches(names(regions), regexpr("[^//]+$", names(regions))))
for (i in 1:length(regions)) {
  reg_dat <- grab_dist_data(regions[i])
  reg_dat$Species <- spec_name[i]
  total_regions[[i]] <- reg_dat
}
total_regions <- do.call(rbind, total_regions)
head(total_regions, 50)
```

Lastly, we save the data tot he `data` directory for later use.

```{r save_data}
save(total_regions, file = "data/total_regions.rda")
```

##References
